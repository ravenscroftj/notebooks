{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citation classifier excite\n",
    "\n",
    "\n",
    "GIT repo: `git clone git@nopro.be:james/excite.git` \n",
    "\n",
    "Implementation of LSTM that can classify citation strings into regions denoting author, article title, journal, volume, year, pages, doi, notes and some other classes. There are 13 classes in total.\n",
    "\n",
    "## Preparing Data regions\n",
    "\n",
    "The following function takes citations that have been annotated and builds a mapping of character to classes. Since neural networks are also completely numerical constructs, we create an alphabet that maps numerical indices in a vector to letters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lxml.etree as ET\n",
    "from collections import Counter\n",
    "\n",
    "def prepare_data(citefile):\n",
    "\n",
    "    #a list of all citations in training data\n",
    "    citations = []\n",
    "    #letters is essentially our alphabet used to map alphanumerical chars to integers\n",
    "    letters = set()\n",
    "    #this is a map of example classes found in the training data\n",
    "    classes = Counter()\n",
    "\n",
    "    with open(citefile) as f:\n",
    "\n",
    "        for line in f.readlines():\n",
    "            #add citation element to beginning and end of each example line so that we can parse into xml doc\n",
    "            root = ET.XML(\"<citation>\" + line.replace(\"&\",\"&amp;\") + \"</citation>\")\n",
    "            cite = \"\"\n",
    "            regions = []\n",
    "\n",
    "            #iterate over child elements of our citation doc\n",
    "            for el in root.iterchildren():\n",
    "                classes[el.tag] = 1\n",
    "                regions.append( (el.tag, len(cite)) )\n",
    "                cite += el.text.replace(\"&amp;\",\"&\")\n",
    "\n",
    "            #letters is a set so we just union against our new citation string to get unique chars used\n",
    "            letters = letters.union( set(list(cite)) )\n",
    "            citations.append( (cite, regions) )\n",
    "\n",
    "    return citations, sorted(classes.keys()), sorted(letters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this function to build test and training sets. We can import data from the training sets attached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total classes found: 13\n",
      "Classes:  ['author', 'booktitle', 'date', 'editor', 'institution', 'journal', 'location', 'note', 'pages', 'publisher', 'tech', 'title', 'volume']\n"
     ]
    }
   ],
   "source": [
    "#extract training data from examples\n",
    "citations, classes, alphabet = prepare_data(\"excite/data/citeseerx.tagged.txt\")\n",
    "\n",
    "# Find out how many classes there were in the training data as a sanity check\n",
    "print ( str.format(\"Total classes found: {}\",len(classes) ) )\n",
    "print (\"Classes: \", classes)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture of Network\n",
    "\n",
    "Now that we have an idea of the number of classes we can start to plan network architecture.\n",
    "\n",
    "### Network input\n",
    "\n",
    "We are using a 5 character context window over our time series (which is essentially moving the context window along one character at a time. We need a context window function, $c(s,t)$ which given an input citation $s$ and an offset, $t$ creates a context window for network input, $x$\n",
    "\n",
    "So you might expect the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abbey, A. & Andrews, F. M.  Modeling the psychological determinants of life quality.  Social Indicators Research  16, 1,  1985.\n",
      "x for t=1:  ['A', 'b', 'b', 'e', 'y']\n",
      "x for t=2:  ['b', 'b', 'e', 'y', ',']\n"
     ]
    }
   ],
   "source": [
    "#given a citation string that looks like this\n",
    "citation = citations[0][0].strip()\n",
    "\n",
    "print (citation)\n",
    "\n",
    "x_t_1 = list(citation[0:5])\n",
    "\n",
    "print (\"x for t=1: \", x_t_1)\n",
    "\n",
    "x_t_2 = list(citation[1:6])\n",
    "\n",
    "print (\"x for t=2: \", x_t_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to make sure that the character we're interested in (which aligns with offset $t$ is at the centre of this matrix. The first character in this string is 'A' so we pad it with emptystring like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x for t=1:  ['', '', 'A', 'b', 'b']\n",
      "x for t=2:  ['', 'A', 'b', 'b', 'e']\n"
     ]
    }
   ],
   "source": [
    "lpadded = 2 * [''] + list(citation) + 2 * ['']\n",
    "\n",
    "x_t_1 = lpadded[0:5]\n",
    "\n",
    "x_t_2 = lpadded[1:6]\n",
    "\n",
    "print (\"x for t=1: \", x_t_1)\n",
    "\n",
    "print (\"x for t=2: \", x_t_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is that neural networks are completely numerical and strings must be encoded as numbers in order to be passed in. Therefore we use the alphabet collected above to map our x values to something more RNN friendly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded x for t=1 [-1 -1 24 55 55]\n",
      "Encoded x for t=2 [-1 24 55 55 58]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def amap(letters):\n",
    "    return np.array([ alphabet.index(x) if x in alphabet else -1 for x in letters])\n",
    "\n",
    "print( \"Encoded x for t=1\", amap(x_t_1) )\n",
    "print( \"Encoded x for t=2\", amap(x_t_2) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can wrap all this up in a function very easily now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1 -1 24 55 55]\n",
      "[-1 -1 24 55 55]\n"
     ]
    }
   ],
   "source": [
    "def cite_to_input(citestring, t):\n",
    "    lpadded = 2 * [''] + list(citestring) + 2 * ['']\n",
    "    cwin = lpadded[ 0 + t : 5 + t]\n",
    "    return amap(cwin)\n",
    "\n",
    "print(amap(x_t_1))\n",
    "print (cite_to_input(citation, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "### Network output\n",
    "    \n",
    "\n",
    "The output will be a vector of values between 0,1 as wide as the number of classes. \n",
    "\n",
    "The aim is to use back propogation through time to make the input match with an output of all zeroes except the correct class in the vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "\n",
    "#we know the input context window is 5 \n",
    "n_in = 5\n",
    "#the output is the number of classes (currently 13)\n",
    "n_out = len(classes)\n",
    "\n",
    "#hidden units somewhere between input and output - we'll try 10 for now \n",
    "n_hidden = 10\n",
    "\n",
    "# we set up the various layers of the network \n",
    "\n",
    "#the input layer is just a vector - yes it is 5 wide but we don't care about this yet\n",
    "L_in = T.dvector('x_in')\n",
    "\n",
    "# States is a vector of memory unit values\n",
    "S_lstm = theano.shared(np.zeros(n_hidden))\n",
    "\n",
    "# we want to store the previous state of the LSTM output for future calculations ( y(t-1))\n",
    "y_lstm_t = theano.shared(np.zeros(n_hidden))\n",
    "\n",
    "#output layer is a vector of values which will eventually be len(classes) wide\n",
    "L_out = T.dvector('y_out')\n",
    "\n",
    "# set up all the weights - these are matrices that weight values map connections between layers\n",
    "# W[l,m] is the weight of connection from unit m to unit l\n",
    "\n",
    "#weights for hidden layer internal connections \n",
    "W_hh = theano.shared(np.random.randn(n_hidden,n_hidden) * 0.001)\n",
    "#weights for input to hidden connections\n",
    "W_hi  = theano.shared( np.random.randn(n_hidden, n_in) * 0.001 )\n",
    "#weights for hidden to out\n",
    "W_oh  = theano.shared( np.random.randn(n_out, n_hidden ) * 0.001 )\n",
    "\n",
    "#weight for input gate function \n",
    "W_in = theano.shared(value=np.random.randn() * 0.001, name='W_in')\n",
    "#weight for forget gate function\n",
    "W_phi = theano.shared(value=np.random.randn() * 0.001, name='W_phi')\n",
    "#weight for output gate function\n",
    "W_out = theano.shared(value=np.random.randn() * 0.001, name='W_out')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "\n",
    "This section is an implementation of Forward prop described in ['Learning Precise Timing with LSTM Recurrent Networks' - Gers et al. 2002](http://www.jmlr.org/papers/volume3/gers02a/gers02a.pdf)\n",
    "\n",
    "Each time the sequence advances (or each time $t$ goes up by 1) we have to forward propagate the input $x$ through the network and calculate output. Here we define what that looks like.\n",
    "\n",
    "### Input Gate\n",
    "\n",
    "$$ z_{c^v_j}(t) = \\sum_m w_{c^v_j m} y_m (t-1)  $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', 'A', 'b', 'b']\n",
      "[-1 -1 24 55 55]\n",
      "[-0.00604327  0.03790514 -0.04192091  0.08768305  0.0034655  -0.05841105\n",
      "  0.15559771 -0.00504544  0.0811      0.00613676]\n"
     ]
    }
   ],
   "source": [
    "z_lstm =  W_hi.dot(L_in) + W_hh.dot( y_lstm_t )\n",
    "\n",
    "#compile a function to demo what this looks like\n",
    "f = theano.function( [L_in], z_lstm)\n",
    "\n",
    "print(x_t_1)\n",
    "print(amap(x_t_1))\n",
    "print( f( amap(x_t_1) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ z_{in_j}(t)=\\sum_m w_{in_j m} y_m (t-1)$$\n",
    "\n",
    "$$y_{in_j}(t)= f_{in_j}(z_{in_j}(t))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', 'A', 'b', 'b']\n",
      "[-1 -1 24 55 55]\n",
      "0.5070567957478449\n"
     ]
    }
   ],
   "source": [
    "y_in_lstm =  T.nnet.sigmoid( L_in.dot(W_in).sum() + y_lstm_t.dot( W_in ).sum() )\n",
    "\n",
    "#compile a function to demo what this looks like\n",
    "f = theano.function( [L_in], y_in_lstm )\n",
    "\n",
    "print(x_t_1)\n",
    "print(amap(x_t_1))\n",
    "print( f( amap(x_t_1) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forget gate\n",
    "\n",
    "\n",
    "\n",
    "$$ z_{\\varphi_j}(t)=\\sum_m w_{\\varphi_j m} y_m (t-1)$$\n",
    "\n",
    "$$y_{\\varphi_j}(t)= f_{\\varphi_j}(z_{\\varphi_j}(t))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', 'A', 'b', 'b']\n",
      "[-1 -1 24 55 55]\n",
      "0.5225861167139391\n"
     ]
    }
   ],
   "source": [
    "y_phi_lstm = T.nnet.sigmoid( L_in.dot(W_phi).sum() + y_lstm_t.dot( W_phi ).sum() )\n",
    "\n",
    "#compile a function to demo what this looks like\n",
    "f = theano.function([L_in], y_phi_lstm )\n",
    "\n",
    "print(x_t_1)\n",
    "print(amap(x_t_1))\n",
    "print( f( amap(x_t_1) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell state\n",
    "\n",
    "If $ t \\gt 0$ then it is calculated like so:\n",
    "\n",
    "$$s_{c^v_j} (t) =  y_{\\varphi j}(t)s_{c^v_j} (t-t) + y_{in_j}(t) g(z_{c^v_j}(t))  $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', 'A', 'b', 'b']\n",
      "[-1 -1 24 55 55]\n",
      "[-0.00306428  0.01922006 -0.02125628  0.04446028  0.00175721 -0.02961772\n",
      "  0.07889688 -0.00255833  0.04112231  0.00311169]\n"
     ]
    }
   ],
   "source": [
    "s_lstm = ( y_phi_lstm * S_lstm) + (y_in_lstm * z_lstm )\n",
    "\n",
    "f = theano.function([L_in],s_lstm)\n",
    "\n",
    "print(x_t_1)\n",
    "print(amap(x_t_1))\n",
    "print(f( amap(x_t_1) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output activation function\n",
    "\n",
    "$$ z_{out_j}(t)=\\sum_m w_{out_j m} y_m (t-1)$$\n",
    "\n",
    "$$y_{out_j}(t)= f_{out_j}(z_{out_j}(t))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', 'A', 'b', 'b']\n",
      "[-1 -1 24 55 55]\n",
      "0.535537531526583\n"
     ]
    }
   ],
   "source": [
    "y_out_lstm =  T.nnet.sigmoid( L_in.dot(W_out).sum() + y_lstm_t.dot( W_out ).sum() )\n",
    "\n",
    "#compile a function to demo what this looks like\n",
    "f = theano.function( [L_in], y_out_lstm )\n",
    "\n",
    "print(x_t_1)\n",
    "print(amap(x_t_1))\n",
    "print( f( amap(x_t_1) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Layer Output value\n",
    "\n",
    "$$ y_{c^v_j}(t) =y_{out_j}(t) s_{c^v_j}(t) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', 'A', 'b', 'b']\n",
      "[-1 -1 24 55 55]\n",
      "[-0.00164104  0.01029306 -0.01138354  0.02381015  0.00094105 -0.0158614\n",
      "  0.04225224 -0.00137008  0.02202254  0.00166642]\n"
     ]
    }
   ],
   "source": [
    "y_lstm = y_out_lstm * s_lstm\n",
    "\n",
    "f = theano.function([L_in],y_lstm)\n",
    "\n",
    "print(x_t_1)\n",
    "print(amap(x_t_1))\n",
    "print(f( amap(x_t_1) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network output\n",
    "\n",
    "The final layer is a standard densely connected neural network layer len(classes) in size. y_k(t)$ denotes the final output of the network.\n",
    "\n",
    "$$ y_k(t) = f_k(z_k(t))$$\n",
    "$$ z_k(t) = \\sum_m w_{km} y_m(t) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', 'A', 'b', 'b']\n",
      "[-1 -1 24 55 55]\n",
      "[[ 0.07691889  0.07692877  0.07692734  0.07692417  0.07692361  0.07692447\n",
      "   0.07692004  0.07692868  0.07692535  0.07692062  0.07691622  0.07692303\n",
      "   0.07691879]]\n"
     ]
    }
   ],
   "source": [
    "z_k = W_oh.dot(y_lstm)\n",
    "y_k = T.nnet.softmax( z_k )\n",
    "\n",
    "f = theano.function([L_in], y_k)\n",
    "\n",
    "print(x_t_1)\n",
    "print(amap(x_t_1))\n",
    "print(f( amap(x_t_1) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backwards Pass\n",
    "\n",
    "\n",
    "### References\n",
    "\n",
    "Implementation of Backwards Propagation through time (BPTT) - truncated gradients.\n",
    "\n",
    "Implementation details described in \n",
    "  * ['Long Short Term Memory' Hochreiter & Schmidhuber 1997](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf)\n",
    "  * ['Learning to Forget: Continual Prediction with LSTM' Gers et al. 1999](http://www.felixgers.de/papers/FgGates-ICANN99.pdf) \n",
    "  * ['Learning Precise Timing with LSTM Recurrent Networks' - Gers et al. 2002](http://www.jmlr.org/papers/volume3/gers02a/gers02a.pdf)\n",
    "  \n",
    "### Outputs and Output Gate\n",
    "  \n",
    "#### Defining our targets $t(t)$\n",
    "First we define targets $t(t)$ which represent the correct answers.\n",
    "\n",
    "Our input is a 5 wide vector of chars with the character we are interested in at its centre as demonstrated by the following where X is the letter we care about: [ \\_ , \\_, X \\_, \\_   ]. \n",
    "\n",
    "Our examples are divided up into boundaries so we just need to find the correct class for time $t$ (or rather offset $t$ if it helps to think that way).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('author', 0), ('title', 28), ('journal', 86), ('volume', 114), ('date', 122)]\n",
      "author\n",
      "title\n",
      "journal\n",
      "volume\n",
      "date\n"
     ]
    }
   ],
   "source": [
    "def class_for_offset(regions, t):\n",
    "    \n",
    "    for cls, idx in sorted(regions, key=lambda x: x[1], reverse=True):\n",
    "        if t >= idx:\n",
    "            return cls\n",
    "    else:\n",
    "        return regions[-1][0]\n",
    "\n",
    "\n",
    "citestring, regions = citations[0]\n",
    "\n",
    "print (regions)\n",
    "\n",
    "print( class_for_offset(regions, 12))\n",
    "print( class_for_offset(regions, 30))\n",
    "print( class_for_offset(regions, 90))\n",
    "print( class_for_offset(regions, 115))\n",
    "print( class_for_offset(regions, 123))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then its as simple as replacing the class name with an index to make 1 where everything else is zero\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "def output_vec(regions, t):\n",
    "    cls = class_for_offset(regions,t)\n",
    "    out = np.zeros(len(classes))\n",
    "    out[classes.index(cls)] = 1\n",
    "    return out\n",
    "\n",
    "\n",
    "citestring, regions = citations[0]\n",
    "\n",
    "print( output_vec(regions,12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have $t(t)$ we can start building BPTT.\n",
    "\n",
    "#### Minimising error and gradient descent\n",
    "\n",
    "Our error function is root mean squared error of our expected and actual outputs\n",
    "\n",
    "$$E(t) = \\frac{1}{2}\\sum_ke_k(t)^2$$  \n",
    "$$e_k(t) := t_k(t) - y_k(t)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [-1 -1  0 24 55]\n",
      "Desired Output: [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "Actual Output: [[ 0.07691939  0.07692331  0.07692656  0.07692295  0.07692385  0.07692451\n",
      "   0.07692307  0.07692718  0.07692553  0.07692164  0.07691796  0.07692391\n",
      "   0.07692015]]\n",
      "Error E(t)= 0.46154214912007907\n"
     ]
    }
   ],
   "source": [
    "t_k = T.dvector('t_k')\n",
    "E_t = 0.5 * T.sqr((t_k - y_k)).sum()\n",
    "\n",
    "f = theano.function([L_in, t_k], E_t)\n",
    "y = theano.function([L_in], y_k)\n",
    "\n",
    "citation, regions = citations[0]\n",
    "input  = cite_to_input(citation,0)\n",
    "target = output_vec(regions, 0)\n",
    "print (\"Input:\", input)\n",
    "print (\"Desired Output:\",target)\n",
    "print (\"Actual Output:\", y(input))\n",
    "print (\"Error E(t)=\", f(input, target))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to minimize $E(t)$ by doing gradient descent weight changes $\\Delta w_{lm}$ using learning rate $\\alpha$:\n",
    "\n",
    "$$ \\Delta w_{lm}(t) = \\alpha \\delta_k (t) y_m(t-1) $$\n",
    "\n",
    "$$\\delta_k (t) = f'_k(z_k(t))e_k(t)$$\n",
    "\n",
    "$$ e_k(t) = t_k(t) - y_k(t) $$\n",
    "\n",
    "Remembering that:\n",
    "  * $\\Delta w_{lm}(t)$ is the change in weight between current cell $l$ and its input cell $m$\n",
    "  * $y^m(t-1)$ is feeder cell $m$'s previous input value (for $t-1$) and which we store in our program as `y_lstm_t`\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "e_k = t_k - y_k\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "delta_k = T.grad(E_t,y_k) * e_k\n",
    "\n",
    "delta_W_oh = learning_rate * T.grad(E_t, W_oh)\n",
    "\n",
    "f = theano.function([L_in, t_k], W_oh - delta_W_oh) #theano.function([L_in,t_k], learning_rate * ( delta_k.reshape((1,13)).dot(y_lstm_t) ) ) \n",
    "\n",
    "\n",
    "#print (\"W_oh: \", W_oh.get_value())\n",
    "#print (\"W_oh - delta_W_oh: \", f(input,target))\n",
    "\n",
    "updates = []\n",
    "updates.append((W_oh, W_oh - delta_W_oh))\n",
    "updates.append( (y_lstm_t, y_lstm) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Gate\n",
    "\n",
    "$$\\delta_{out_j}(t) = f'_{out_j}(z_{out_j}(t)) \\Big( \\sum_{v=1}^{S_j} s_{c^v_j}(t) \\sum_k W_{kc^v_j} \\delta_k(t) \\Big) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_out: 0.00107871378158\n",
      "W_oh - delta_W_out:  0.00013607913091728883\n"
     ]
    }
   ],
   "source": [
    "\n",
    "delta_W_out = T.grad(E_t, W_out)\n",
    "\n",
    "f = theano.function([L_in, t_k], delta_W_out) #theano.function([L_in,t_k], learning_rate * ( delta_k.reshape((1,13)).dot(y_lstm_t) ) ) \n",
    "\n",
    "print (\"W_out:\", W_out.get_value())\n",
    "print (\"W_oh - delta_W_out: \", f(input,target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single step\n",
    "\n",
    "Now that we have weights defined, we can define what it means to do a single step given time, $t$. This involves forward propogating new (and recycled inputs) and then backpropogating error signals.\n",
    "\n",
    "Here we define a python function, $step$ that carries out these processes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def step(x,y):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
