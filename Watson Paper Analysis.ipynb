{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scientific Paper Text Analysis\n",
    "\n",
    "CoreSC is a sentence-level annotation scheme for scientific papers that *\"recognizes the main components of scientific investigations as represented in articles\"* [Liakata et al 2012](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3315721/?report=classic). \n",
    "\n",
    "![alt text](assets/coresc.jpg \"Examples of CoreSC designations\")\n",
    "\n",
    "We spent months building a CRF classifier that gets ~51% accuracy over 11 classes of sentence. \n",
    "\n",
    "I spent 2 days training and testing an NLC instance with 63% accuracy out of the box.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8501 sentences in this corpus\n",
      "Scientific Concepts:\n",
      "There are 168 Goa ( 1.9762380896365135% )sentences\n",
      "There are 1839 Exp ( 21.63274908834255% )sentences\n",
      "There are 631 Obs ( 7.4226561580990476% )sentences\n",
      "There are 54 Other ( 0.6352193859545936% )sentences\n",
      "There are 1826 Bac ( 21.47982590283496% )sentences\n",
      "There are 4 Mod ( 0.04705328784848841% )sentences\n",
      "There are 293 Obj ( 3.446653334901776% )sentences\n",
      "There are 586 Con ( 6.893306669803552% )sentences\n",
      "There are 305 Mot ( 3.5878131984472414% )sentences\n",
      "There are 929 Met ( 10.928126102811433% )sentences\n",
      "There are 1718 Res ( 20.209387130925773% )sentences\n",
      "There are 148 Hyp ( 1.7409716503940715% )sentences\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from collections import Counter,defaultdict\n",
    "\n",
    "\n",
    "\n",
    "sentences = defaultdict(lambda: [])\n",
    "for rootdir, dirs, files in os.walk(\"consensus_annotated/\"):\n",
    "\n",
    "    for file in files:\n",
    "        fullname = os.path.join(rootdir, file)\n",
    "\n",
    "        #open and parse the paper\n",
    "        tree = ET.parse(fullname)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        #lets find all sentences in the paper\n",
    "        for sentEl in root.iter(\"s\"):\n",
    "            annoArt = sentEl.find('CoreSc1')\n",
    "            text = sentEl.find(\"text\")\n",
    "            if annoArt != None:\n",
    "                sentences[annoArt.get('type')].append(\"\".join(sentEl.itertext()))\n",
    "\n",
    "            else:\n",
    "                    sentences['Other'].append(\"\".join(sentEl.itertext()))\n",
    "\n",
    "\n",
    "\n",
    "sentcount = sum( [len(x) for x in sentences.values() ])\n",
    "print(\"There are {} sentences in this corpus\".format(sentcount))\n",
    "\n",
    "print (\"Scientific Concepts:\")\n",
    "for lbl,sents in sentences.items():\n",
    "\n",
    "    pc = len(sents) / sentcount * 100\n",
    "    print(\"There are {} {} ( {}% )sentences\".format(len(sents),lbl, pc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goa 117 51\n",
      "Exp 1287 552\n",
      "Obs 441 190\n",
      "Other 37 17\n",
      "Bac 1278 548\n",
      "Mod 2 2\n",
      "Obj 205 88\n",
      "Con 410 176\n",
      "Mot 213 92\n",
      "Met 650 279\n",
      "Res 1202 516\n",
      "Hyp 103 45\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "TRAIN = 0.7\n",
    "\n",
    "def write_gt(filename, gt_dict):\n",
    "    \n",
    "    with open(filename,encoding='utf-8',mode=\"w\") as f:\n",
    "        csvw = csv.writer(f, lineterminator='\\n')\n",
    "        \n",
    "        for lbl,sents in gt_dict.items():\n",
    "            for sent in sents:\n",
    "                if len(sent) > 1024:\n",
    "                    continue\n",
    "                else:\n",
    "                    csvw.writerow([sent,lbl])\n",
    "\n",
    "def split_train_test(sentences, proportion):\n",
    "    \n",
    "    indices = range(0,len(sentences))\n",
    "    \n",
    "    samplesize = math.floor(len(indices) * proportion)\n",
    "    \n",
    "    trainidx = random.sample(indices,samplesize)\n",
    "    \n",
    "    testidx = list(set(indices) - set(trainidx))\n",
    "    \n",
    "    return [sentences[x] for x in trainidx], [sentences[y] for y in testidx]\n",
    "    \n",
    "train = { x:None for x in sentences.keys() }\n",
    "test = { x:None for x in sentences.keys() }\n",
    "\n",
    "for idx, sents in sentences.items():\n",
    "    train[idx],test[idx] = split_train_test(sentences[idx], TRAIN)\n",
    "    print(idx,len(train[idx]),len(test[idx]))\n",
    "    \n",
    "write_gt(\"train.csv\", train)\n",
    "write_gt(\"test.csv\",test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training a classifier\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint https://gateway.watsonplatform.net/natural-language-classifier/api with username 59b52427-a680-49cf-aa23-50531cbcf1d4 \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"creds.json\") as f:\n",
    "    creds = json.load(f)['credentials']\n",
    "    username,password = creds['username'], creds['password']\n",
    "    endpoint = creds['url']\n",
    "    \n",
    "    print(\"Using endpoint {} with username {} \".format(endpoint,username))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"classifier_id\" : \"3a84d1x62-nlc-1038\",\n",
      "  \"name\" : \"CoreSC Classifier\",\n",
      "  \"language\" : \"en\",\n",
      "  \"created\" : \"2016-04-28T08:28:00.125Z\",\n",
      "  \"url\" : \"https://gateway.watsonplatform.net/natural-language-classifier/api/v1/classifiers/3a84d1x62-nlc-1038\",\n",
      "  \"status\" : \"Training\",\n",
      "  \"status_description\" : \"The classifier instance is in its training phase, not yet ready to accept classify requests\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "meta = { \"language\":\"en\", \"name\":\"CoreSC Classifier\"  } \n",
    "\n",
    "r = requests.post(endpoint + \"/v1/classifiers\", \n",
    "              auth=(username,password),\n",
    "             files={ \"training_data\" : open(\"train.csv\"), \"training_metadata\" : json.dumps(meta)   })\n",
    "\n",
    "print (r.text)\n",
    "\n",
    "classifierID = r.json()['classifier_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'CoreSC Classifier', 'url': 'https://gateway.watsonplatform.net/natural-language-classifier/api/v1/classifiers/3a84d1x62-nlc-1038', 'classifier_id': '3a84d1x62-nlc-1038', 'created': '2016-04-28T08:28:00.125Z', 'language': 'en', 'status': 'Available', 'status_description': 'The classifier instance is now available and is ready to take classifier requests.'}\n"
     ]
    }
   ],
   "source": [
    "classifierID=\"3a84d1x62-nlc-1038\" #\"3a84d1x62-nlc-1027\" #3a84cfx63-nlc-886\"\n",
    "r = requests.get(endpoint + \"/v1/classifiers/\" + classifierID, \n",
    "              auth=(username,password))\n",
    "\n",
    "print (r.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "b'{\\n  \"classifier_id\" : \"3a84d1x62-nlc-1038\",\\n  \"url\" : \"https://gateway.watsonplatform.net/natural-language-classifier/api/v1/classifiers/3a84d1x62-nlc-1038\",\\n  \"text\" : \"Aryl hydrocarbon receptor (AhR) agonists suppress interleukin-6 expression by bone marrow stromal cells: an immunotoxicology study\",\\n  \"top_class\" : \"Bac\",\\n  \"classes\" : [ {\\n    \"class_name\" : \"Bac\",\\n    \"confidence\" : 0.9851534941025172\\n  }, {\\n    \"class_name\" : \"Obj\",\\n    \"confidence\" : 0.0037460404592158138\\n  }, {\\n    \"class_name\" : \"Hyp\",\\n    \"confidence\" : 0.0021243958290101186\\n  }, {\\n    \"class_name\" : \"Other\",\\n    \"confidence\" : 0.0018484181209979776\\n  }, {\\n    \"class_name\" : \"Con\",\\n    \"confidence\" : 0.001608292345229142\\n  }, {\\n    \"class_name\" : \"Goa\",\\n    \"confidence\" : 0.0013573938282026969\\n  }, {\\n    \"class_name\" : \"Mod\",\\n    \"confidence\" : 8.968194007939619E-4\\n  }, {\\n    \"class_name\" : \"Res\",\\n    \"confidence\" : 8.561750682276065E-4\\n  }, {\\n    \"class_name\" : \"Mot\",\\n    \"confidence\" : 6.851031715196856E-4\\n  }, {\\n    \"class_name\" : \"Met\",\\n    \"confidence\" : 6.01987106813002E-4\\n  } ]\\n}'\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from IPython.display import display\n",
    "from ipywidgets import FloatProgress\n",
    "\n",
    "tps = Counter()\n",
    "fns = Counter()\n",
    "\n",
    "\n",
    "fp = FloatProgress(min=0, max=sum(len(x) for x in test.values()))\n",
    "display(fp)\n",
    "resuls = []\n",
    "\n",
    "f = open(\"results.txt\",\"w\")\n",
    "\n",
    "for label,sents in test.items():\n",
    "    \n",
    "    for sent in sents:\n",
    "        r = requests.get(\n",
    "            endpoint + \"/v1/classifiers/\" + classifierID + \"/classify\",\n",
    "            auth=(username,password),\n",
    "            params={\"text\" : sent}\n",
    "        )\n",
    "        try:\n",
    "            result = r.json()['classes'][0]['class_name']\n",
    "\n",
    "            results.append((label, result))\n",
    "\n",
    "            f.write(\"{},{}\\n\".format(label,result))\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        fp.value += 1\n",
    "\n",
    "f.close()\n",
    "    \n",
    "\n",
    "print (r)\n",
    "print (r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21647 34168\n",
      "Label\t\tPrecision\tRecall\t\tF-measure\n",
      "Goa\t\t78.47%\t\t43.04%\t\t55.59%\n",
      "Exp\t\t81.15%\t\t82.59%\t\t81.87%\n",
      "Obs\t\t68.47%\t\t64.65%\t\t66.50%\n",
      "Other\t\t93.75%\t\t88.24%\t\t90.91%\n",
      "Bac\t\t59.81%\t\t71.75%\t\t65.24%\n",
      "Mod\t\t64.03%\t\t66.28%\t\t65.13%\n",
      "Obj\t\t58.77%\t\t50.63%\t\t54.40%\n",
      "Con\t\t71.50%\t\t52.08%\t\t60.26%\n",
      "Mot\t\t61.35%\t\t36.73%\t\t45.95%\n",
      "Met\t\t54.18%\t\t53.92%\t\t54.05%\n",
      "Res\t\t55.51%\t\t68.86%\t\t61.47%\n",
      "Hyp\t\t58.87%\t\t36.87%\t\t45.34%\n",
      "63.35460079606649\n"
     ]
    }
   ],
   "source": [
    "correct = sum([1 for true,exp in results if true == exp])\n",
    "total = len(results)\n",
    "\n",
    "print (correct,total)\n",
    "\n",
    "tp = Counter()\n",
    "fp = Counter()\n",
    "fn = Counter()\n",
    "\n",
    "for true, predictedLabel in results:\n",
    "    #logger.info(\"%s, %s, %s\", true, predictedLabel, probability)\n",
    "    if true == predictedLabel:\n",
    "        tp[true] += 1\n",
    "    else:\n",
    "        fp[predictedLabel] += 1\n",
    "        fn[true] += 1\n",
    "\n",
    "print (\"Label\\t\\tPrecision\\tRecall\\t\\tF-measure\")\n",
    "for label in sentences.keys():\n",
    "    #logger.info(label)\n",
    "    if tp[label] == 0:\n",
    "        prec = 0\n",
    "        rec = 0\n",
    "    else:\n",
    "        prec = tp[label] / (tp[label] + fp[label])\n",
    "        rec = tp[label] / (tp[label] + fn[label])\n",
    "\n",
    "    if (prec + rec) > 0:\n",
    "        fm = (2 * prec * rec ) / (prec + rec)\n",
    "    else:\n",
    "        fm = 0\n",
    "\n",
    "    #logger.info('prec: %d tp / (%d tp + %d fp) = %f', tp[label], tp[label], fp[label], prec)\n",
    "    #logger.info('rec: %d tp / (%d tp + %d fn) = %f', tp[label], tp[label], fn[label], rec)\n",
    "    #logger.info('F-measure: %f',fm)\n",
    "\n",
    "   \n",
    "    print(\"{}\\t\\t{:.2%}\\t\\t{:.2%}\\t\\t{:.2%}\".format(label,prec,rec,fm))\n",
    "    #csvw.writerow([label, prec, rec, fm])\n",
    "    \n",
    "\n",
    "print (correct/total*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alchemy enrichment and classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from alchemyapi import AlchemyAPI\n",
    "\n",
    "import os\n",
    "import hashlib\n",
    "import json\n",
    "from IPython.display import display\n",
    "from ipywidgets import FloatProgress\n",
    "\n",
    "fp = FloatProgress(min=0, max=sum(len(x) for x in sentences.values()))\n",
    "\n",
    "alchemyapi = AlchemyAPI()\n",
    "\n",
    "display(fp)\n",
    "\n",
    "for idx, sents in sentences.items():\n",
    "    \n",
    "    datapath = os.path.join(\"data\",idx)\n",
    "    \n",
    "    for sent in sents:\n",
    "        fp.value += 1\n",
    "        m = hashlib.md5()\n",
    "        m.update(sent.encode('utf-8'))\n",
    "        \n",
    "        if not os.path.exists(datapath):\n",
    "            os.makedirs(datapath)\n",
    "            \n",
    "        sent_file = os.path.join(datapath, m.hexdigest())\n",
    "        \n",
    "        if os.path.exists(sent_file) and (os.path.getsize(sent_file) > 69):\n",
    "            continue\n",
    "\n",
    "        result = alchemyapi.combined(\"text\", sent, options={ \"showSourceText\":1})\n",
    "\n",
    "        with open(sent_file,\"w\") as f:\n",
    "            json.dump(result, f)\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Managing Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifiers': [{'url': 'https://gateway.watsonplatform.net/natural-language-classifier/api/v1/classifiers/3a84d1x62-nlc-1027', 'name': 'CoreSC Classifier', 'classifier_id': '3a84d1x62-nlc-1027', 'created': '2016-04-28T06:44:28.406Z', 'language': 'en'}, {'url': 'https://gateway.watsonplatform.net/natural-language-classifier/api/v1/classifiers/3a84d1x62-nlc-1038', 'name': 'CoreSC Classifier', 'classifier_id': '3a84d1x62-nlc-1038', 'created': '2016-04-28T08:28:00.125Z', 'language': 'en'}, {'url': 'https://gateway.watsonplatform.net/natural-language-classifier/api/v1/classifiers/3a84cfx63-nlc-886', 'name': 'CoreSC Classifier', 'classifier_id': '3a84cfx63-nlc-886', 'created': '2016-04-27T09:07:18.634Z', 'language': 'en'}]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print (requests.get(endpoint + \"/v1/classifiers\", \n",
    "              auth=(username,password)).json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "print (requests.delete(endpoint + \"/v1/classifiers/3a84d1x62-nlc-962\", \n",
    "              auth=(username,password)).json())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
