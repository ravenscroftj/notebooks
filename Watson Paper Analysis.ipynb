{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scientific Paper Text Analysis\n",
    "\n",
    "CoreSC is an annotation scheme for scientific papers \n",
    "\n",
    "\n",
    "IMAGE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 40180 sentences in this corpus\n",
      "Scientific Concepts:\n",
      "There are 3858 Exp ( 9.60179193628671% )sentences\n",
      "There are 8407 Res ( 20.923344947735192% )sentences\n",
      "There are 1358 Obj ( 3.3797909407665507% )sentences\n",
      "There are 3656 Mod ( 9.09905425584868% )sentences\n",
      "There are 541 Mot ( 1.3464410154305626% )sentences\n",
      "There are 5410 Obs ( 13.464410154305625% )sentences\n",
      "There are 783 Hyp ( 1.948730711796914% )sentences\n",
      "There are 4285 Met ( 10.664509706321553% )sentences\n",
      "There are 3646 Con ( 9.0741662518666% )sentences\n",
      "There are 629 Goa ( 1.5654554504728722% )sentences\n",
      "There are 7607 Bac ( 18.93230462916874% )sentences\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from collections import Counter,defaultdict\n",
    "\n",
    "\n",
    "\n",
    "sentences = defaultdict(lambda: [])\n",
    "for rootdir, dirs, files in os.walk(\"consensus_annotated/\"):\n",
    "\n",
    "    for file in files:\n",
    "        fullname = os.path.join(rootdir, file)\n",
    "\n",
    "        #open and parse the paper\n",
    "        tree = ET.parse(fullname)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        #lets find all sentences in the paper\n",
    "        for sentEl in root.iter(\"s\"):\n",
    "            annoArt = sentEl.find('CoreSc1')\n",
    "            text = sentEl.find(\"text\")\n",
    "            if annoArt != None:\n",
    "                sentences[annoArt.get('type')].append(\"\".join(text.itertext()))\n",
    "\n",
    "            elif text != None:\n",
    "                    sentences['Other'].append(\"\".join(text.itertext()))\n",
    "\n",
    "\n",
    "\n",
    "sentcount = sum( [len(x) for x in sentences.values() ])\n",
    "print(\"There are {} sentences in this corpus\".format(sentcount))\n",
    "\n",
    "print (\"Scientific Concepts:\")\n",
    "for lbl,sents in sentences.items():\n",
    "\n",
    "    pc = len(sents) / sentcount * 100\n",
    "    print(\"There are {} {} ( {}% )sentences\".format(len(sents),lbl, pc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exp 3086 772\n",
      "Res 6725 1682\n",
      "Obj 1086 272\n",
      "Mod 2924 732\n",
      "Mot 432 109\n",
      "Obs 4328 1082\n",
      "Hyp 626 157\n",
      "Met 3428 857\n",
      "Con 2916 730\n",
      "Goa 503 126\n",
      "Bac 6085 1522\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "TRAIN = 0.8\n",
    "\n",
    "def write_gt(filename, gt_dict):\n",
    "    \n",
    "    with open(filename,encoding='utf-8',mode=\"w\") as f:\n",
    "        csvw = csv.writer(f, lineterminator='\\n')\n",
    "        \n",
    "        for lbl,sents in gt_dict.items():\n",
    "            for sent in sents:\n",
    "                csvw.writerow([lbl,sent])\n",
    "\n",
    "def split_train_test(sentences, proportion):\n",
    "    \n",
    "    indices = range(0,len(sentences))\n",
    "    \n",
    "    samplesize = math.floor(len(indices) * proportion)\n",
    "    \n",
    "    trainidx = random.sample(indices,samplesize)\n",
    "    \n",
    "    testidx = list(set(indices) - set(trainidx))\n",
    "    \n",
    "    return [sentences[x] for x in trainidx], [sentences[y] for y in testidx]\n",
    "    \n",
    "train = { x:None for x in sentences.keys() }\n",
    "test = { x:None for x in sentences.keys() }\n",
    "\n",
    "for idx, sents in sentences.items():\n",
    "    train[idx],test[idx] = split_train_test(sentences[idx], TRAIN)\n",
    "    print(idx,len(train[idx]),len(test[idx]))\n",
    "    \n",
    "write_gt(\"train.csv\", train)\n",
    "write_gt(\"test.csv\",test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
