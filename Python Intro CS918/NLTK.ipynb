{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK \n",
    "\n",
    "\n",
    "NLTK or Natural Language Toolkit is the *de-facto* standard python module for building and running natural language processing applications in the language.\n",
    "\n",
    "It consists of a collection of widely used datasets and algorithms for language processing applications such as tokenizers, part-of-speech taggers, stopword sets, standard text sets and trainable machine-learning algorithms.\n",
    "\n",
    "Using the toolkit requires a reasonable working knowledge of Python which hopefully by studying the [preceeding exercises](https://brainsteam.co.uk/wiki/public:phd:teaching:cs918) you will have by now.\n",
    "\n",
    "In this exercise we will use a number of the skills we have already examined to do some analysis on the [ART corpus](https://www.aber.ac.uk/en/cs/research/cb/projects/art/art-corpus/).\n",
    "\n",
    "## About the dataset\n",
    "\n",
    "If you have been through the XML exercise then you will already have been briefly introduced to the corpus. It consists of 225 biochemistry papers that have been broken up into individual sentences.\n",
    "\n",
    "Each sentence has also been annotated with a label that describes the core scientific concept (CoreSC) that the sentence encapsulates.\n",
    "\n",
    "For example, a sentence might be labelled \"Motivation\" if it explains why the author is carrying out a particular study or it might be labelled \"Hypothesis\" if it discusses expected results of a study. You can find out more about the annotation scheme specification [in this paper by Liakata et al. 2010](http://www.lrec-conf.org/proceedings/lrec2010/pdf/644_Paper.pdf).\n",
    "\n",
    "## What can we find out?\n",
    "\n",
    "Let's play with the data! We know that a particular sentence has a particular label. From this, can we make any assumptions about the words in the sentences? Are there some words that occur more frequently in some types of sentences?\n",
    "\n",
    "Let's find out...\n",
    "\n",
    "## Preparing the data\n",
    "\n",
    "The first thing we need to do is extract a list of sentences and their respective CoreSC label. I have downloaded the ART corpus `tar.gz` file and placed it in the assets folder in this project. The following code iterates over the contained folders, finding all XML files and parsing them.\n",
    "\n",
    "First we import requisite libraries..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os #we haven't used this one before. Check out the docs https://docs.python.org/3/library/os.html\n",
    "\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function for parsing the paper, extracting sentences and returning a list of tuples (sentence text,label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_paper(filename):\n",
    "    \n",
    "    #open and parse the paper\n",
    "    tree = ET.parse(\"assets/b414459g_mode2.xml\")\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    sents = []\n",
    "    \n",
    "    #iterate through sentences\n",
    "    for sent in root.iter(\"s\"):\n",
    "        annoArt = sent.find('annotationART')\n",
    "        id = sent.get(\"sid\")\n",
    "        text = \"\".join(annoArt.itertext())\n",
    "        coreSC = annoArt.get(\"type\")\n",
    "        sents.append( (filename, id, text,coreSC) )\n",
    "    \n",
    "    return sents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We iterate through all papers in the ART corpus using [`os.walk`](https://docs.python.org/3/library/os.html#os.walk) which recursively steps through all files and subdirectories in a given directory, allowing you to process any file of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspected 225 files\n",
      "Collected 16650 sentences\n"
     ]
    }
   ],
   "source": [
    "art_path = \"assets/ART_Corpus\"\n",
    "\n",
    "all_sents = []\n",
    "\n",
    "filecount = 0\n",
    "\n",
    "# this layer of the for loop iterates through each subdirectory of the given path\n",
    "# producing a list of directories and files in that immediate directory.\n",
    "# root represents the (sub)directory currently being inspected\n",
    "for root, dirs, files in os.walk(art_path):\n",
    "    \n",
    "    #we loop through the list of files in the current subdirectory\n",
    "    for file in files:\n",
    "        \n",
    "        if file.endswith(\".xml\"):\n",
    "            filecount += 1\n",
    "            #we use os.path.join to concatenate the directory name and file name safely\n",
    "            #with respect to slashes in the path\n",
    "            fullpath = os.path.join(root,file)\n",
    "            \n",
    "            #we parse the file and keep the sentences\n",
    "            all_sents += extract_paper(file)\n",
    "            \n",
    "#lets get a total count of sentences collected and files examined\n",
    "print(\"Inspected {} files\".format(filecount))\n",
    "print (\"Collected {} sentences\".format(len(all_sents)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, let's inspect 5 sentences in the array and make sure that they are of the format (filename, id, some text, coresc). We use the standard library module [`random`](https://docs.python.org/3.0/library/random.html) to pick 5 from the list at random. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('b315252a_mode2.xml', '20', 'A schematic diagram of the experiment is shown in Fig. 1(a).', 'Exp')\n",
      "('b406989g_mode2.xml', '46', 'Fig. 2 illustrates three CERS spectral fingerprints obtained from optically trapped water droplets of different sizes.', 'Obs')\n",
      "('b407283a_mode2.xml', '45', 'The fingerprint of resonant wavelengths can be used to determine the droplet size with nanometre precision.4,5', 'Bac')\n",
      "('b509164k_mode2.xml', '28', 'The backscattered Raman light from the trapped droplet is collimated by the objective lens, passed through the filter, and focussed onto the entrance slit of a 0.5 m spectrograph (1200 g/mm grating) coupled with a CCD, with a spectral resolution of 0.05 nm.', 'Exp')\n",
      "('b404146a_mode2.xml', '1', 'Control and characterisation of a single aerosol droplet in a single-beam gradient-force optical trap', 'Obj')\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "randsents = random.sample(all_sents,5)\n",
    "\n",
    "for senttuple in randsents:\n",
    "    print (senttuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks right. Out of curiosity, lets see how many of each CoreSC type there are and plot a Pie chart using matplotlib.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-0e0be8b0a988>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#read about the counter object on the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#collections doc page (https://docs.python.org/3/library/collections.html#collections.Counter)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/james/projects/notebooks/env/lib/python3.4/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mmagic\u001b[1;34m(self, arg_s)\u001b[0m\n\u001b[0;32m   2334\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2335\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2336\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2338\u001b[0m     \u001b[1;31m#-------------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/james/projects/notebooks/env/lib/python3.4/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[1;34m(self, magic_name, line)\u001b[0m\n\u001b[0;32m   2255\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'local_ns'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2256\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2257\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2258\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2259\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-106>\u001b[0m in \u001b[0;36mmatplotlib\u001b[1;34m(self, line)\u001b[0m\n",
      "\u001b[1;32m/home/james/projects/notebooks/env/lib/python3.4/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/james/projects/notebooks/env/lib/python3.4/site-packages/IPython/core/magics/pylab.py\u001b[0m in \u001b[0;36mmatplotlib\u001b[1;34m(self, line)\u001b[0m\n\u001b[0;32m     98\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Available matplotlib backends: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbackends_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m             \u001b[0mgui\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_matplotlib\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgui\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_show_matplotlib_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgui\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/james/projects/notebooks/env/lib/python3.4/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36menable_matplotlib\u001b[1;34m(self, gui)\u001b[0m\n\u001b[0;32m   3118\u001b[0m         \"\"\"\n\u001b[0;32m   3119\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpylabtools\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3120\u001b[1;33m         \u001b[0mgui\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_gui_and_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgui\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpylab_gui_select\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3122\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mgui\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/james/projects/notebooks/env/lib/python3.4/site-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mfind_gui_and_backend\u001b[1;34m(gui, gui_select)\u001b[0m\n\u001b[0;32m    237\u001b[0m     \"\"\"\n\u001b[0;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 239\u001b[1;33m     \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mgui\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mgui\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'auto'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#read about the counter object on the \n",
    "#collections doc page (https://docs.python.org/3/library/collections.html#collections.Counter)\n",
    "from collections import Counter \n",
    "\n",
    "lbl_tally = Counter([ lbl for file, id, text, lbl in all_sents])\n",
    "\n",
    "total = len(all_sents)\n",
    "\n",
    "percentages = { x: (lbl_tally[x] / total * 100) for x in lbl_tally }\n",
    "\n",
    "labels = [ x for x in percentages.keys() ]\n",
    "values = [ percentages[x] for x in labels]\n",
    "\n",
    "plt.pie(values, \n",
    "        labels=labels, \n",
    "        labeldistance=1.3,\n",
    "        autopct=\"%1.1f%%\", \n",
    "        pctdistance=1.1,\n",
    "        colors=[\"white\",\"green\",\"yellow\",\"blue\",\"red\", \"purple\", \"orange\",\"pink\",\"grey\",\"lightblue\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that there is quite a lot of variation in the types of sentences and their respective representation within the corpus. Background takes up over a quarter of the total number of sentences and hypothesis a measly 1.4%.\n",
    "\n",
    "We need to remember this in our experiments - all CoreSCs are not born equal - some are more likely to occur than others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word distributions\n",
    "\n",
    "Let's look at which words are most likely to occur in different sentence types. We build a dictionary of words by parsing and tokenizing every single sentence in our collection. We store a record of each word's occurence in each CoreSC so that we can build a frequency distribution table later. We will depend upon NLTK heavily for this work.\n",
    "\n",
    "### Tokenizing sentences\n",
    "First we use `nltk.word_tokenize` to break down each sentence into a series of \"words\" and build a dictionary. Then we examine the top 10 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 33975), ('of', 23850), (',', 18000), ('.', 13275), ('a', 12375), ('and', 10350), ('to', 9225), ('droplet', 7875), ('in', 7650), ('with', 7650)]\n"
     ]
    }
   ],
   "source": [
    "dictionary = Counter()\n",
    "\n",
    "for file, id, text, lbl in all_sents:\n",
    "    \n",
    "    words = nltk.word_tokenize(text)\n",
    "    dictionary.update(words)\n",
    "    \n",
    "print(dictionary.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Filtering out common words\n",
    "\n",
    "Unsurprisingly, the most common word is 'the' and second most common is 'of'. In fact until we get down to 'droplet' there really aren't any surprises. After 'droplet' we have with and in which are also what you might consider \"common\" words in the English language. \n",
    "\n",
    "In natural language processing, these are what we call \"stopwords\" and they are typically filtered out before we do anything interesting with the data to remove excess noise. After all, we already know that 'the' etc are common words - we aren't interested in this.\n",
    "\n",
    "NLTK comes, by default, with a set of stopwords we can use to filter on. It does not incorporate punctutation so we manually append '.' and ',' to the stopwords list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2435 stopwords\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords as stopwordloader\n",
    "\n",
    "stopwords = stopwordloader.words()\n",
    "\n",
    "stopwords.extend(['.',',',')','('])\n",
    "\n",
    "print (\"There are {} stopwords\".format(len(stopwords)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This modified next step could take a bit longer because Python now has to run a filter operation on every single word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('droplet', 7875), ('droplets', 4275), ('aerosol', 3825), ('water', 3600), ('laser', 3375), ('size', 3375), ('scattering', 3150), ('CERS', 2925), ('wavelengths', 2250), ('optical', 2250)]\n"
     ]
    }
   ],
   "source": [
    "dictionary = Counter()\n",
    "\n",
    "for file, id, text, lbl in all_sents:\n",
    "    \n",
    "    words = [ word for word in nltk.word_tokenize(text) if word.lower() not in stopwords ]\n",
    "    dictionary.update(words)\n",
    "    \n",
    "print(dictionary.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much more interesting, we can see that the top words all look much more relevent to the biochemistry domain and there are no longer any punctuation marks in the list.\n",
    "\n",
    "Now we attempt to capture the most common words in each separate CoreSC. We initialise counters for each CoreSC label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 words for Mot\n",
      "[('droplet', 900), ('single', 900), ('two', 675), ('scattering', 675), ('optical', 675)]\n",
      "Top 5 words for Met\n",
      "[('axis', 225), ('trap', 225), ('beam', 225), ('three', 225), ('using', 225)]\n",
      "Top 5 words for Exp\n",
      "[('aerosol', 1800), ('droplet', 1575), ('trap', 900), ('trapped', 900), ('beam', 675)]\n",
      "Top 5 words for Bac\n",
      "[('droplet', 1350), ('size', 1350), ('scattering', 1125), ('force', 1125), ('wavelengths', 1125)]\n",
      "Top 5 words for Obj\n",
      "[('droplet', 1125), ('aerosol', 450), ('droplets', 450), ('signal', 225), ('characterisation', 225)]\n",
      "Top 5 words for Con\n",
      "[('CERS', 900), ('droplet', 900), ('optical', 675), ('droplets', 450), ('size', 450)]\n",
      "Top 5 words for Mod\n",
      "[('droplet', 450), ('gravitational', 450), ('absorbance', 450), ('particle', 450), ('light', 450)]\n",
      "Top 5 words for Res\n",
      "[('droplet', 675), ('trapping', 675), ('increase', 450), ('temperature.5', 450), ('heating', 450)]\n",
      "Top 5 words for Hyp\n",
      "[('droplet', 450), ('resonant', 450), ('lead', 225), ('work', 225), ('K.', 225)]\n",
      "Top 5 words for Obs\n",
      "[('droplets', 1350), ('water', 1125), ('Fig', 900), ('scattering', 675), ('±', 675)]\n"
     ]
    }
   ],
   "source": [
    "counters = { x: Counter() for x in set([lbl for file, id, text, lbl in all_sents]) }\n",
    "\n",
    "for file, id, text, lbl in all_sents:\n",
    "    \n",
    "    words = [ word for word in nltk.word_tokenize(text) if word.lower() not in stopwords ]\n",
    "    counters[lbl].update(words)\n",
    "    \n",
    "for lbl in counters:\n",
    "    print(\"Top 5 words for {}\".format(lbl))\n",
    "    print(counters[lbl].most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD/IDF\n",
    "\n",
    "You can see that there is a lot of crossover for some of these words - 'droplet' appears in the top 5 for 8 of the 10 CoreSC concepts represented in the ART corpus. What would be more interesting is a list of the top words that appear in a CoreSC **and** that are mostly distinct to that CoreSC.\n",
    "\n",
    "Here we use a measurement called Term Frequency-Intra-Document Frequency (TF-IDF). The Term Frequency (TF) is the number of times a term appears in the document (or CoreSC) you are inspecting. The Intra-Document Frequency (IDF) is the specivity of the term or word. By multiplying TF by IDF we identify how 'unique' the term is to the document under scrutiny. \n",
    "\n",
    "A high TF-IDF suggests that the word or term is very relevent to the current document. A low TF-IDF suggests that the word is common across the whole corpus. \n",
    "\n",
    "Here we use this principle to identify which words in the dictionary are most relevent when working with a specific CoreSC category.\n",
    "\n",
    "TF is normally just calculated as the frequency of the term within the document you are interested in. However if one document is longer than the rest (or in our case one CoreSC is represented more strongly than another) then words in that class (i.e. words in Background (Bac) will probably have a stronger natural TF than words in Methodology (Met) purely because Background forms 28% of the corpus and Methodology just over 1%. In order to normalise this, we use augmented frequency defined as:\n",
    "\n",
    "$$ tf(t,d) = 0.5 + 0.5 over { f_td } { max( f_{t`,d} : t`  )  } $$\n",
    "\n",
    "NB: TF-IDF can also be used to identify and filter out stopwords. However, by removing common stopwords using a list as we did in the steps above, we have reduced the complexity of the calculations pre-emptively and hopefully reduced overall noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs [('varying', 0.02592592592592593), ('O–H', 0.02592592592592593), ('polarisation', 0.02592592592592593), ('comprised', 0.02592592592592593), ('spectra', 0.02592592592592593)]\n",
      "Mot [('opportunity', 0.02777777777777778), ('biological', 0.02777777777777778), ('transformations', 0.02777777777777778), ('examined', 0.02777777777777778), ('first', 0.02777777777777778)]\n",
      "Met [('dimensional', 0.044444444444444446), ('plane', 0.044444444444444446), ('constrained', 0.044444444444444446), ('forming', 0.044444444444444446), ('horizontal', 0.044444444444444446)]\n",
      "Mod [('2πa/λ', 0.03333333333333333), ('limit', 0.03333333333333333), ('upper', 0.03333333333333333), ('axial', 0.03333333333333333), ('grade', 0.03333333333333333)]\n",
      "Bac [('diameter.1', 0.02592592592592593), ('feedback', 0.02592592592592593), ('role', 0.02592592592592593), ('research', 0.02592592592592593), ('ethanol', 0.02592592592592593)]\n",
      "Con [('coupling', 0.02777777777777778), ('range', 0.02777777777777778), ('increasing', 0.02777777777777778), ('suggesting', 0.02777777777777778), ('symptomatic', 0.02777777777777778)]\n",
      "Res [('control', 0.02962962962962963), ('suggests', 0.02962962962962963), ('applying', 0.02962962962962963), ('0.047', 0.02962962962962963), ('correlates', 0.02962962962962963)]\n",
      "Exp [('514.5', 0.025), ('brightfield', 0.025), ('systematically', 0.025), ('falls', 0.025), ('second', 0.025)]\n",
      "Obj [('possible', 0.02666666666666667), ('characterisation', 0.02666666666666667), ('Control', 0.02666666666666667), ('equilibrating', 0.02666666666666667), ('3', 0.02666666666666667)]\n",
      "Hyp [('occur.21', 0.03333333333333333), ('noted', 0.03333333333333333), ('orders', 0.03333333333333333), ('suggest', 0.03333333333333333), ('K.', 0.03333333333333333)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tfidfs =  { label : {} for label in counters }\n",
    "\n",
    "def tf(word, coresc):\n",
    "    f_td = counters[coresc].most_common(1)[0][1]\n",
    "    return 0.5 + (0.5 * counters[coresc][word] / f_td)\n",
    "\n",
    "def idf(word):\n",
    "    \n",
    "    N = len(counters)\n",
    "\n",
    "    return N / dictionary[word] \n",
    "\n",
    "tfidf = lambda word,coresc: tf(word,coresc) * idf(word)\n",
    "    \n",
    "\n",
    "for word in [ w for w in dictionary if dictionary[w] > 1]:\n",
    "    for coresc in counters:\n",
    "        tfidfs[coresc][word] = tfidf(word,coresc)\n",
    "        \n",
    "for coresc in tfidfs:\n",
    "    topwords = sorted(tfidfs[coresc], key= lambda x: tfidfs[coresc][x], reverse=True)[0:5] \n",
    "    topscores = [ (word, tfidfs[coresc][word]) for word in topwords ]\n",
    "    print ( coresc, topscores )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
