{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK \n",
    "\n",
    "\n",
    "NLTK or Natural Language Toolkit is the *de-facto* standard python module for building and running natural language processing applications in the language.\n",
    "\n",
    "It consists of a collection of widely used datasets and algorithms for language processing applications such as tokenizers, part-of-speech taggers, stopword sets, standard text sets and trainable machine-learning algorithms.\n",
    "\n",
    "Using the toolkit requires a reasonable working knowledge of Python which hopefully by studying the [preceeding exercises](https://brainsteam.co.uk/wiki/public:phd:teaching:cs918) you will have by now.\n",
    "\n",
    "In this exercise we will use a number of the skills we have already examined to do some analysis on the [ART corpus](https://www.aber.ac.uk/en/cs/research/cb/projects/art/art-corpus/).\n",
    "\n",
    "## About the dataset\n",
    "\n",
    "If you have been through the XML exercise then you will already have been briefly introduced to the corpus. It consists of 225 biochemistry papers that have been broken up into individual sentences.\n",
    "\n",
    "Each sentence has also been annotated with a label that describes the core scientific concept (CoreSC) that the sentence encapsulates.\n",
    "\n",
    "For example, a sentence might be labelled \"Motivation\" if it explains why the author is carrying out a particular study or it might be labelled \"Hypothesis\" if it discusses expected results of a study. You can find out more about the annotation scheme specification [in this paper by Liakata et al. 2010](http://www.lrec-conf.org/proceedings/lrec2010/pdf/644_Paper.pdf).\n",
    "\n",
    "## What can we find out?\n",
    "\n",
    "Let's play with the data! We know that a particular sentence has a particular label. From this, can we make any assumptions about the words in the sentences? Are there some words that occur more frequently in some types of sentences?\n",
    "\n",
    "Let's find out...\n",
    "\n",
    "## Preparing the data\n",
    "\n",
    "The first thing we need to do is extract a list of sentences and their respective CoreSC label. I have downloaded the ART corpus `tar.gz` file and placed it in the assets folder in this project. The following code iterates over the contained folders, finding all XML files and parsing them.\n",
    "\n",
    "First we import requisite libraries..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os #we haven't used this one before. Check out the docs https://docs.python.org/3/library/os.html\n",
    "\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function for parsing the paper, extracting sentences and returning a list of tuples (sentence text,label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_paper(filename):\n",
    "    \n",
    "    #open and parse the paper\n",
    "    tree = ET.parse(\"assets/b414459g_mode2.xml\")\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    sents = []\n",
    "    \n",
    "    #iterate through sentences\n",
    "    for sent in root.iter(\"s\"):\n",
    "        annoArt = sent.find('annotationART')\n",
    "        id = sent.get(\"sid\")\n",
    "        text = \"\".join(annoArt.itertext())\n",
    "        coreSC = annoArt.get(\"type\")\n",
    "        sents.append( (filename, id, text,coreSC) )\n",
    "    \n",
    "    return sents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We iterate through all papers in the ART corpus using [`os.walk`](https://docs.python.org/3/library/os.html#os.walk) which recursively steps through all files and subdirectories in a given directory, allowing you to process any file of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspected 225 files\n",
      "Collected 16650 sentences\n"
     ]
    }
   ],
   "source": [
    "art_path = \"assets/ART_Corpus\"\n",
    "\n",
    "all_sents = []\n",
    "\n",
    "filecount = 0\n",
    "\n",
    "# this layer of the for loop iterates through each subdirectory of the given path\n",
    "# producing a list of directories and files in that immediate directory.\n",
    "# root represents the (sub)directory currently being inspected\n",
    "for root, dirs, files in os.walk(art_path):\n",
    "    \n",
    "    #we loop through the list of files in the current subdirectory\n",
    "    for file in files:\n",
    "        \n",
    "        if file.endswith(\".xml\"):\n",
    "            filecount += 1\n",
    "            #we use os.path.join to concatenate the directory name and file name safely\n",
    "            #with respect to slashes in the path\n",
    "            fullpath = os.path.join(root,file)\n",
    "            \n",
    "            #we parse the file and keep the sentences\n",
    "            all_sents += extract_paper(file)\n",
    "            \n",
    "#lets get a total count of sentences collected and files examined\n",
    "print(\"Inspected {} files\".format(filecount))\n",
    "print (\"Collected {} sentences\".format(len(all_sents)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, let's inspect 5 sentences in the array and make sure that they are of the format (filename, id, some text, coresc). We use the standard library module [`random`](https://docs.python.org/3.0/library/random.html) to pick 5 from the list at random. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('b314172a_mode2.xml', '38', 'Q was measured as 0.07 ± 0.02 and 0.1 ± 0.04 for water droplets trapped using the water and oil immersion objectives, respectively, and 0.45 ± 0.12 for decane droplets with the oil immersion objective.', 'Obs')\n",
      "('b501029b_mode2.xml', '71', 'The results from the coagulation measurement shown in Fig. 5 illustrate that the additive volumes of the two individual droplets and the volume of the coagulated droplet are in agreement to within ±3 × 10−19 m3.', 'Obs')\n",
      "('b403191c_mode2.xml', '54', 'Perhaps the most important issue to address is the influence of the laser on the droplet temperature through absorption.20', 'Obj')\n",
      "('b315089e_mode2.xml', '15', 'Conversely, the gradient force acts to draw the dielectric sphere towards the beam focus and is proportional to the gradient of the light intensity.', 'Bac')\n",
      "('b403191c_mode2.xml', '68', 'The position of each droplet could be controlled independently enabling the coagulation of the two droplets to be studied.', 'Obj')\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "randsents = random.sample(all_sents,5)\n",
    "\n",
    "for senttuple in randsents:\n",
    "    print (senttuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks right. Out of curiosity, lets see how many of each CoreSC type there are and plot a Pie chart using matplotlib.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
